---
title: "Bellabeat Notebook"
author: "Fedor Anashchenkov"
date: "2023-02-04"
output: html_document
---

# Ask

### Business task

Using FitBit dataset, identify key user patterns in utilizing a fitness tracker and implement them into the Bellabeat marketing strategy for the product called Leaf. It is the Bellabeat's classic wellness tracker that can be worn as a bracelet, necklace, or clip. The Leaf tracker connects to the Bellabeat app to track activity, sleep, and stress.

### Preliminary observations

The original dataset consists of 18 files, each in a .csv format. All tables share the same column "Id". Also, there is a date and time column in every table.

However, the time range has three different dimensions: seconds (HR only), minutes, hours and days. It means that not all tables are initially compatible with each other, and additional work may be required to get them to a single time dimension.

Also, the time format in a single table differs between 12 and 24 hours models, so there is a need for additional cleaning prior to the analysis.

# **Prepare**

After revising spreadsheets, I decide to take some of them for the analysis: combined daily activity, three hourly tables that I will try to merge, and the table on weight. For some tables, I fixed the time format and removed few columns in Google Sheets beforehand. There, I also added data on daily burn of calories from another spreadsheet to the daily activity table.

### Libraries

```{r}
library(dplyr)
library(ggplot2)
library(janitor)
library(rmarkdown)
library(tidyr)
library(stringr)
library(corrplot)
```

### Import

```{r}
hc <- read.csv("/Bellabeat/Fitabase/hourlyCalories.csv")
hi <- read.csv("/Bellabeat/Fitabase/hourlyIntensities.csv")
hs <- read.csv("/Bellabeat/Fitabase/hourlySteps.csv")
da <- read.csv("/Bellabeat/Fitabase/dailyActivity_1.csv")
weight <- read.csv("/Bellabeat/Fitabase/weightLogInfo_1.csv")
```

# **Clean**

### Hourly activity

Before the merge, I need to check if unique Ids match.

```{r}
unique(hc$Id)
```

There are 33 different users.

Next, I import hourly intensity spreadsheet, removing the "AverageIntensity" column. Checked distinct Ids, as well. They seem to match all values from the "hourlyCalories" spreadsheet. 

```{r}
unique(hc$Id)
hi <- hi[,-4]
```

Finally, checking Ids in the hourly steps spreadsheet.

```{r}
unique(hs$Id)
```

This table has the same users. To doublecheck, I use the "intersect" function. As it allows to compare only two objects, two iterations are required.

```{r}
intersect(hc$Id,hi$Id)
intersect(hc$Id,hs$Id)
```

My assumption is proved, all 33 Ids match. Next, I check date values. Due to a complexity of the chosen date format (date + time), it is better to try the "setdiff" function, so I would be able to see distinct values.

```{r}
setdiff(hc$ActivityHour,hi$ActivityHour)
setdiff(hc$ActivityHour,hs$ActivityHour)
```

No values found. It means that both columns in all three tables match precisely. Thus, we have two columns by whose I could merge all three tables. The only detail: while tables on hourly calories and intensity have no zero values, steps spreadsheet contains numerous zero values.

### Daily activity

I check table's unique Ids to see if the data relates to the same 33 users.

Now, I turn to the daily activity table, deleting some columns in Google Sheets and cleaning the "ActivityDate" column. I checked unique Ids after importing, and 

```{r}
unique(da$Id)
```

They appeared to match those in hourly tables. It means that we analyze same users, which is important for the integrity of data.

### Weight info

In the weight table, column with weight values in lbs was deleted in Google Sheets. Another noticeable detail is that the data was added manually and contains only 67 values, which limits the scope. Still, we could try to search for insights there as this would provide a missing piece.

```{r}
unique(weight$Id)
intersect(weight$Id,da$Id)
```

Weight spreadhseet contains data only from 8 users, but they are included in the broad group of users in other tables.

# **Process**

### Hourly activity

I start with joining three "hourly" spreadsheets using the "left_join" function from the dplyr library.

```{r}
ha <- hc %>% left_join(select(hi, Id, ActivityHour, TotalIntensity), by = c("Id", "ActivityHour")) %>% left_join(select(hs, Id, ActivityHour, StepTotal), by = c("Id", "ActivityHour"))
```

The objective is to find any patterns and outliers in the data on activity across the day among all users. This requires removing the date stamps, as we only need an hour. 

```{r}
ha[c('Date', 'Hour')] <- str_split_fixed(ha$ActivityHour, ' ', 2)
ha <- ha[,-2,5]
```

Next, I convert the "Hour" column to a 24-hour format.

```{r}
ha$Hour[ha$Hour == '1:00:00 AM'] <- '1'
ha$Hour[ha$Hour == '2:00:00 AM'] <- '2'
ha$Hour[ha$Hour == '3:00:00 AM'] <- '3'
ha$Hour[ha$Hour == '4:00:00 AM'] <- '4'
ha$Hour[ha$Hour == '5:00:00 AM'] <- '5'
ha$Hour[ha$Hour == '6:00:00 AM'] <- '6'
ha$Hour[ha$Hour == '7:00:00 AM'] <- '7'
ha$Hour[ha$Hour == '8:00:00 AM'] <- '8'
ha$Hour[ha$Hour == '9:00:00 AM'] <- '9'
ha$Hour[ha$Hour == '10:00:00 AM'] <- '10'
ha$Hour[ha$Hour == '11:00:00 AM'] <- '11'
ha$Hour[ha$Hour == '12:00:00 PM'] <- '12'
ha$Hour[ha$Hour == '1:00:00 PM'] <- '13'
ha$Hour[ha$Hour == '2:00:00 PM'] <- '14'
ha$Hour[ha$Hour == '3:00:00 PM'] <- '15'
ha$Hour[ha$Hour == '4:00:00 PM'] <- '16'
ha$Hour[ha$Hour == '5:00:00 PM'] <- '17'
ha$Hour[ha$Hour == '6:00:00 PM'] <- '18'
ha$Hour[ha$Hour == '7:00:00 PM'] <- '19'
ha$Hour[ha$Hour == '8:00:00 PM'] <- '20'
ha$Hour[ha$Hour == '9:00:00 PM'] <- '21'
ha$Hour[ha$Hour == '10:00:00 PM'] <- '22'
ha$Hour[ha$Hour == '11:00:00 PM'] <- '23'
ha$Hour[ha$Hour == '12:00:00 AM'] <- '24'
```

I change the data type of columns for the easier manipulation during the analysis phase.

```{r}
class(ha$Id) = "character"
class(ha$Hour) = "numeric"
```

Now it's time to summarize all metrics by the Hour.

```{r}
ha <- ha %>% group_by(Hour) %>% summarise(across(c(Calories, TotalIntensity, StepTotal), sum))
```

### Daily Activity

We need to check patterns in daily activity. However, having an exact date creates numerous distinct values in the column. The good solution would be to convert dates into weekdays.

```{r}
da <- da %>% mutate( Weekday = weekdays(as.Date(ActivityDate, "%m/%d/%Y")))
da <- da[,-2]
```

We will also need the data on total amount of active minutes during a day, so I create a new column, summarizing all values across three corresponding columns.

```{r}
da <- da %>% mutate(ActiveMinutesTotal = LightlyActiveMinutes + FairlyActiveMinutes + VeryActiveMinutes)
```

To sort weekdays in a correct order, I decide to use factor levels. Next, I summarize variables by a weekday. 


```{r}
da$Weekday <- factor(da$Weekday, levels=c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))

da <- da %>% group_by(Weekday) %>% summarise(across(c(TotalSteps, Calories, TotalDistance, VeryActiveDistance, ModeratelyActiveDistance, LightActiveDistance, ActiveMinutesTotal, VeryActiveMinutes, FairlyActiveMinutes, LightlyActiveMinutes, SedentaryMinutes), sum))
```

### Average daily activity

Here, I stumbled upon an issue. What if users didn't record their data evenly throughout a week? What if they used bracelets during a specific activity (let's say, sports), which took place on specific weekdays? For this reason, I decided to import the original dataset on daily activity once again and check how many instances each weekday has.    

```{r}
da_2 <- read.csv("/Bellabeat/Fitabase/dailyActivity_1.csv")

da_2 <- da_2 %>% mutate( Weekday = weekdays(as.Date(ActivityDate, "%m/%d/%Y")))
da_2 <- da_2[,-2]

n_distinct(da_2$Weekday)

rle(sort(da_2$Weekday))
```

Indeed, Tuesday, Wednesday and Thursday occur more than other days. It may give us wrong picture when comparing summarized activity between different weekdays. That's why I decide to also create a spreadsheet with mean values of given metrics for the daily activity.

```{r}

da_2 <- da_2 %>% mutate(ActiveMinutesTotal = LightlyActiveMinutes + FairlyActiveMinutes + VeryActiveMinutes)

da_mean <- da_2 %>% group_by(Weekday) %>% summarise(across(c(TotalSteps, TotalDistance, ActiveMinutesTotal, Calories, LightActiveDistance, ModeratelyActiveDistance, VeryActiveDistance, LightlyActiveMinutes, FairlyActiveMinutes, VeryActiveMinutes, SedentaryMinutes), mean))

da_mean$Weekday <- factor(da_mean$Weekday, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))

da_mean <- da_mean[order(da_mean$Weekday), ]

da_mean <- da_mean %>% rename("AverageSteps" = "TotalSteps", "AverageDistance" = "TotalDistance", "AvgActiveMinutesTotal" = "ActiveMinutesTotal", "AverageCalories" = "Calories")
```

### Weight and BMI

Only the weight table is left for processing. To make it meaningful, we need to connect the data to the numbers on activity and calories burnt from the another copy of "original" daily activity table with the "ActivityDate" column kept.

```{r}
da_3 <- read.csv("/Bellabeat/Fitabase/dailyActivity_1.csv")

da_3 <- da_3 %>% mutate(ActiveMinutesTotal = LightlyActiveMinutes + FairlyActiveMinutes + VeryActiveMinutes)

weight <- weight %>%
  left_join(select(da_3, Id, ActivityDate, TotalSteps, TotalDistance, ActiveMinutesTotal, Calories), by = c("Id", "ActivityDate"))

class(weight$Id) = "character"
```

# **Analyze**

### Hourly activity

I build a graph to show each variable's values in the dataframe by the hour. 

```{r}
ggplot(ha, aes(Hour, Calories)) + geom_bar(stat="identity") + geom_point()

ggplot(ha, aes(Hour, TotalIntensity)) + geom_bar(stat="identity") + geom_point()

ggplot(ha, aes(x=Hour, StepTotal)) + geom_bar(stat="identity") + geom_point()
```

We see that that all 3 variables follow a same pattern: smallest values reside in 01:00 - 04:00 area, then activity starts to increase at 05:00,  peaks first between 12:00 - 14:00 and then between 17:00 - 19:00, successively dropping from 20:00 onwards.

### Total daily activity

```{r}
ggplot(da, aes(Weekday, Calories)) + geom_bar(stat="identity") + geom_point() + geom_text(aes(label = Calories), hjust = 0.5,  vjust = -1)

ggplot(da, aes(Weekday, ActiveMinutesTotal)) + geom_bar(stat="identity") + geom_point() + geom_text(aes(label = ActiveMinutesTotal), hjust = 0.5,  vjust = -1)

ggplot(da, aes(Weekday, TotalSteps)) + geom_bar(stat="identity") + geom_point() + geom_text(aes(label = TotalSteps), hjust = 0.5,  vjust = -1)

ggplot(da, aes(Weekday, TotalDistance)) + geom_bar(stat="identity") + geom_point()
```

Activity is low on Monday, peaks on Tuesday throughout Thursday, drops on Friday, then slightly rises on Saturday and decreases on Sunday. However, the data on calories breaks this pattern, its value is bigger on Saturday than Friday. Maybe that day users perform more intense activity? To answer the question, I explore metrics of intensity levels.

```{r}
ggplot(da, aes(Weekday, VeryActiveDistance)) + geom_bar(stat="identity") + geom_point()

ggplot(da, aes(Weekday, ModeratelyActiveDistance)) + geom_bar(stat="identity") + geom_point()

ggplot(da, aes(Weekday, LightActiveDistance)) + geom_bar(stat="identity") + geom_point()

```

No, all types of distance are higher on Saturday than Friday. Next, checking the minutes by the intensity type. 

```{r}
ggplot(da, aes(Weekday, SedentaryMinutes)) + geom_bar(stat="identity") + geom_point() + geom_text(aes(label = SedentaryMinutes), hjust = 0.5,  vjust = -1)

ggplot(da, aes(Weekday, VeryActiveMinutes)) + geom_bar(stat="identity") + geom_point() + geom_text(aes(label = VeryActiveMinutes), hjust = 0.5,  vjust = -1)

ggplot(da, aes(Weekday, FairlyActiveMinutes)) + geom_bar(stat="identity") + geom_point() + geom_text(aes(label = FairlyActiveMinutes), hjust = 0.5,  vjust = -1)

ggplot(da, aes(Weekday, LightlyActiveMinutes)) + geom_bar(stat="identity") + geom_point() + geom_text(aes(label = LightlyActiveMinutes), hjust = 0.5,  vjust = -1)


```

There are less sedentary minutes on Saturday. This weekday overtakes in other types, except for being a bit less in terms of lightly active minutes. But this still cannot explain the difference in total amount of burnt calories. Let's see if the same picture happens for the mean numbers.

### Average daily activity

```{r}
ggplot(da_mean, aes(Weekday, AverageCalories)) + geom_bar(stat="identity") + geom_point()

ggplot(da_mean, aes(Weekday, AvgActiveMinutesTotal)) + geom_bar(stat="identity") + geom_point()

ggplot(da_mean, aes(Weekday, AverageSteps)) + geom_bar(stat="identity") + geom_point()

ggplot(da_mean, aes(Weekday, AverageDistance)) + geom_bar(stat="identity") + geom_point()
```

Here, all charts with aggregate variables fall into one pattern, including the amount of burnt calories, meaning average data is more concise. Also, activity across a week is also different: Monday actually is, on average, an active day, while Thursday is an outsider, along with Sunday. I tend to trust this outcome more than the summarized data.

### Weight and BMI

This dataframe is a bit tricky. We may assume that one of the main reasons for getting a fitness tracker is to measure a physical effort aimed at losing weight, rather than just tracking daily routines. But we can't define what weight exactly makes a human being "overweight". However, we have data on BMI, which is a pretty common method for measuring an overweight. This makes possible to check if there are any patterns in terms of activity when compared to a certain BMI.

The standard BMI threshold for an overweight is 25. Thus, we build a scatterplot and put a line on the "overweight threshold"

```{r}
ggplot(weight, aes(BMI, Calories, color=Id)) + geom_point(size = 2) + geom_vline(xintercept = 25)

ggplot(weight, aes(BMI, TotalDistance, color=Id)) + geom_point(size = 2) + geom_vline(xintercept = 25)

ggplot(weight, aes(BMI, TotalSteps, color=Id)) + geom_point(size = 2) + geom_vline(xintercept = 25)

ggplot(weight, aes(BMI, ActiveMinutesTotal, color=Id)) + geom_point(size = 2) + geom_vline(xintercept = 25)
```

Two users were more active compared to others. They may had been trying to lose weight and extensively tracked their efforts using the bracelet, but the data is too limited to make broad conclusions. Numbers from more users on BMI is required for the further research. The assumption here is that those behind the "overweight" threshold by the BMI measurement tend to perform more physical activity and are the motivated target audience for fitness trackers.

# **Share**

For the presentation, we will need final dataframes.

```{r}
write.csv(da, "C:\\Bellabeat\\Bellabeat\\rstudio_outcome\\da.csv", row.names=TRUE)
write.csv(da_mean, "C:\\Bellabeat\\Bellabeat\\rstudio_outcome\\da_mean.csv", row.names=TRUE)
write.csv(da_2, "C:\\Bellabeat\\Bellabeat\\rstudio_outcome\\da_2.csv", row.names=TRUE)
write.csv(da_3, "C:\\Bellabeat\\Bellabeat\\rstudio_outcome\\da_3.csv", row.names=TRUE)
write.csv(ha, "C:\\Bellabeat\\Bellabeat\\rstudio_outcome\\ha.csv", row.names=TRUE)
write.csv(weight, "C:\\Bellabeat\\Bellabeat\\rstudio_outcome\\weight.csv", row.names=TRUE)
```

The presentation made and published in [Tableau Public](https://public.tableau.com/app/profile/fedor.anashchenkov/viz/Fitbit_16751203129820/FitBitDataAnalysis).

# **Act**

Based on the findings, following recommendations were made:

1. Create a set of standard activity programs in the app, containing triggers with push notifications on a certain day and hour, based on the findings.
2. Encourage users to add weight values either automatically or manually to gather more data on BMI in the future.
3. If correlation between high activity beyond a certain BMI value is proved, suggest specific use cases for that group of users.
